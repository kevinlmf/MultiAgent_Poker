"""
Exploitative Training Against GTO Opponents
Goal: Train agent to achieve 3 BB/100 hands against GTO players

Setup:
- N-1 players use GTO strategy (baseline)
- 1 learning agent tries to exploit them
- Success = Consistent 3+ BB/100 over 5000+ hands
"""

import os
import sys
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

import argparse
import logging
from pathlib import Path
import numpy as np
import torch

torch.set_num_threads(1)
torch.set_num_interop_threads(1)

from datetime import datetime
from tqdm import tqdm

sys.path.append(str(Path(__file__).parent.parent))

from env import SafePokerEnv, BankrollConstraints
from algorithms.madac_poker_torch import MADACPokerTorch
from agents.gto_agent import GTOAgentPool
from evaluation.ev_metrics import EVEvaluator, HandResult
from evaluation.action_logger import ActionLogger


def setup_logging(log_dir: str):
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"exploitative_{timestamp}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('ExploitativeTraining')


def parse_args():
    parser = argparse.ArgumentParser(description='Train Exploitative Agent vs GTO')

    parser.add_argument('--n_players', type=int, default=6,
                       help='Total players (1 learning + N-1 GTO)')
    parser.add_argument('--big_blind', type=float, default=10.0)
    parser.add_argument('--small_blind', type=float, default=5.0)

    # Training
    parser.add_argument('--max_episodes', type=int, default=20000)
    parser.add_argument('--eval_interval', type=int, default=500, help='Increased for speed')
    parser.add_argument('--eval_hands', type=int, default=200, help='Reduced for speed')
    parser.add_argument('--target_ev', type=float, default=3.0)
    parser.add_argument('--convergence_hands', type=int, default=2000)

    # Learning
    parser.add_argument('--lr_actor', type=float, default=1e-4)
    parser.add_argument('--lr_critic', type=float, default=3e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--batch_size', type=int, default=1024, help='Increased for speed')

    # Paths
    parser.add_argument('--save_dir', type=str, default='checkpoints_exploit/')
    parser.add_argument('--log_dir', type=str, default='logs_exploit/')
    parser.add_argument('--results_dir', type=str, default='results_exploit/')

    parser.add_argument('--seed', type=int, default=42)

    return parser.parse_args()


def play_hand_vs_gto(env, learning_trainer, gto_pool, evaluator, learning_id=0, action_logger=None):
    """
    Play one hand where learning agent plays against GTO opponents

    Args:
        env: Poker environment
        learning_trainer: Learning agent's trainer
        gto_pool: Pool of GTO agents
        evaluator: EV evaluator
        learning_id: ID of learning agent (default 0)

    Returns:
        hand_results: Results for all players
    """
    obs = env.reset()
    done = {i: False for i in range(env.n_players)}

    player_actions = {i: [] for i in range(env.n_players)}
    initial_bankrolls = {i: env.game_state.players[i].bankroll for i in range(env.n_players)}

    step_count = 0
    max_steps = 100

    # Start new hand in logger
    if action_logger:
        action_logger.start_new_hand()

    while not all(done.values()) and step_count < max_steps:
        # Get observations for all players
        obs_array = np.array([obs[i] for i in range(env.n_players)])

        # Learning agent selects action
        learning_obs = obs_array[learning_id:learning_id+1]
        learning_action, _ = learning_trainer.select_actions(learning_obs, deterministic=True)

        # GTO agents select actions
        gto_actions = gto_pool.select_actions(obs_array)

        # Combine actions
        actions = {}
        for i in range(env.n_players):
            if i == learning_id:
                actions[i] = int(learning_action[0])
            else:
                # Map GTO agent index (i-1 if i > learning_id, else i)
                gto_idx = i - 1 if i > learning_id else i
                if gto_idx < len(gto_actions):
                    actions[i] = int(gto_actions[gto_idx])
                else:
                    actions[i] = 0  # Fold as fallback

        # Track actions
        for i, action_idx in actions.items():
            action_name = 'fold' if action_idx == 0 else ('call' if action_idx == 1 else 'raise')
            player_actions[i].append(action_name)

            # Log to CSV
            if action_logger:
                current_bankroll = env.game_state.players[i].bankroll
                pot_size = sum(env.game_state.pot.values()) if hasattr(env.game_state, 'pot') else 0

                action_logger.log_action(
                    player_id=i,
                    position=i,
                    betting_round='preflop',  # Simplified
                    action=action_name,
                    amount=10.0 if action_idx > 0 else 0,  # Simplified
                    pot_before=pot_size,
                    pot_after=pot_size + (10.0 if action_idx > 0 else 0),
                    stack_before=current_bankroll,
                    stack_after=current_bankroll - (10.0 if action_idx > 0 else 0),
                    is_gto_player=(i != learning_id)
                )

        # Step environment
        obs, rewards, done, _ = env.step(actions)
        step_count += 1

    # Record results
    hand_results = []
    for i in range(env.n_players):
        final_bankroll = env.game_state.players[i].bankroll
        profit = final_bankroll - initial_bankrolls[i]
        won = profit > 0 and 'fold' not in player_actions[i]

        hand_result = HandResult(
            player_id=i,
            profit=profit,
            position=i,
            action_sequence=player_actions[i],
            won=won
        )
        hand_results.append(hand_result)

        # Only track learning agent in evaluator
        if i == learning_id:
            evaluator.record_hand(i, hand_result)

    return hand_results


def collect_training_data(env, learning_trainer, gto_pool, n_steps=512, learning_id=0):
    """
    Collect training data with learning agent playing against GTO opponents (optimized)
    """
    states_list = []
    actions_list = []
    rewards_list = []
    dones_list = []
    values_list = []
    safety_costs_list = []

    for _ in range(n_steps // 20):  # Reduce iterations since each hand has multiple steps
        obs = env.reset()
        done = {i: False for i in range(env.n_players)}

        hand_steps = 0
        while not all(done.values()) and hand_steps < 20:
            obs_array = np.array([obs[i] for i in range(env.n_players)])

            # Learning agent
            learning_obs = obs_array[learning_id:learning_id+1]
            learning_action, info = learning_trainer.select_actions(learning_obs, deterministic=False)

            # GTO agents
            gto_actions = gto_pool.select_actions(obs_array)

            # Combine
            actions = {}
            for i in range(env.n_players):
                if i == learning_id:
                    actions[i] = int(learning_action[0])
                else:
                    gto_idx = i - 1 if i > learning_id else i
                    if gto_idx < len(gto_actions):
                        actions[i] = int(gto_actions[gto_idx])
                    else:
                        actions[i] = 0

            next_obs, rewards, done, env_infos = env.step(actions)

            # Store learning agent's experience
            if not done.get(learning_id, True):
                states_list.append(obs[learning_id])
                actions_list.append(actions[learning_id])
                rewards_list.append(rewards.get(learning_id, 0.0))
                dones_list.append(float(done.get(learning_id, False)))
                values_list.append(info[0]['task_value'])
                safety_costs_list.append(
                    env_infos.get(learning_id, {}).get('risk_metrics', {}).get('constraint_value', 0.0)
                )

            obs = next_obs
            hand_steps += 1

    if not states_list:
        # Return dummy data if no data collected
        return {
            'states': np.zeros((1, env.observation_space.shape[0])),
            'actions': np.zeros(1, dtype=np.int64),
            'rewards': np.zeros(1),
            'dones': np.zeros(1),
            'values': np.zeros(1),
            'safety_costs': np.zeros(1)
        }

    return {
        'states': np.array(states_list),
        'actions': np.array(actions_list),
        'rewards': np.array(rewards_list),
        'dones': np.array(dones_list),
        'values': np.array(values_list),
        'safety_costs': np.array(safety_costs_list)
    }


def evaluate_vs_gto(env, learning_trainer, gto_pool, evaluator, n_hands=500, learning_id=0, logger=None, action_logger=None):
    """Evaluate learning agent against GTO opponents"""

    for _ in range(n_hands):
        play_hand_vs_gto(env, learning_trainer, gto_pool, evaluator, learning_id, action_logger)

    metrics = evaluator.compute_metrics(learning_id, window=n_hands)

    if logger:
        logger.info(f"Evaluation over {n_hands} hands:")
        logger.info(f"  EV: {metrics.ev_bb_per_100:.2f} BB/100")
        logger.info(f"  Win Rate: {metrics.win_rate:.2%}")
        logger.info(f"  Sharpe Ratio: {metrics.sharpe_ratio:.4f}")
        logger.info(f"  Max Drawdown: {metrics.max_drawdown:.2f} BB")

        converged, msg = evaluator.check_convergence(
            learning_id,
            min_hands=1000,
            window=500,
            target_ev=3.0,
            tolerance=0.5
        )
        logger.info(f"  Status: {msg}")

    return metrics


def train(args):
    """Main training loop"""

    logger = setup_logging(args.log_dir)
    logger.info("="*70)
    logger.info("EXPLOITATIVE TRAINING vs GTO OPPONENTS")
    logger.info("="*70)
    logger.info(f"Total Players: {args.n_players}")
    logger.info(f"Learning Agents: 1 (Player 0)")
    logger.info(f"GTO Opponents: {args.n_players - 1}")
    logger.info(f"Target: {args.target_ev} BB/100 hands")
    logger.info("="*70)

    # Setup
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)

    # Environment
    constraints = BankrollConstraints(
        min_bankroll=50.0,
        max_variance=1000.0,
        max_drawdown=0.6
    )

    env = SafePokerEnv(
        n_players=args.n_players,
        starting_bankroll=1000.0,
        small_blind=args.small_blind,
        big_blind=args.big_blind,
        constraints=constraints,
        game_type='limit'
    )

    # Learning agent (Player 0)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    learning_id = 0

    learning_trainer = MADACPokerTorch(
        state_dim=state_dim,
        action_dim=action_dim,
        n_agents=1,  # Only 1 learning agent
        lr_actor=args.lr_actor,
        lr_critic=args.lr_critic,
        gamma=args.gamma,
        device='cpu'
    )

    # GTO opponents (Players 1 to N-1)
    gto_pool = GTOAgentPool(
        n_agents=args.n_players - 1,
        small_blind=args.small_blind,
        big_blind=args.big_blind
    )

    logger.info(f"GTO Pool created: {args.n_players - 1} opponents")

    # Evaluator
    evaluator = EVEvaluator(big_blind=args.big_blind, n_players=args.n_players)

    # Action Logger
    action_logger = ActionLogger(output_dir=os.path.join(args.results_dir, 'actions'))

    # Directories
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(args.results_dir, exist_ok=True)

    logger.info("\nStarting training...")

    best_ev = -float('inf')
    converged = False

    pbar = tqdm(range(args.max_episodes), desc="Training vs GTO")

    for episode in pbar:
        # Collect training data
        trajectories = collect_training_data(
            env, learning_trainer, gto_pool,
            n_steps=args.batch_size,
            learning_id=learning_id
        )

        # Training step
        if len(trajectories['states']) > 64:
            metrics = learning_trainer.train_step(trajectories, epochs=3)  # Reduced epochs

            if episode % 100 == 0:  # Less frequent updates
                pbar.set_postfix({
                    'loss': f"{metrics['actor_loss']:.3f}",
                    'lambda': f"{learning_trainer.get_avg_lambda():.2f}"
                })

        # Evaluation
        if episode % args.eval_interval == 0 and episode > 0:
            logger.info(f"\n{'='*70}")
            logger.info(f"Episode {episode} - EVALUATION")
            logger.info(f"{'='*70}")

            eval_metrics = evaluate_vs_gto(
                env, learning_trainer, gto_pool, evaluator,
                n_hands=args.eval_hands,
                learning_id=learning_id,
                logger=logger,
                action_logger=action_logger
            )

            # Save actions CSV periodically (less frequently)
            if episode % 2000 == 0:
                csv_path = action_logger.save_to_csv(f'actions_ep{episode}.csv')
                logger.info(f"Actions saved to {csv_path}")

            # Save best model
            if eval_metrics.ev_bb_per_100 > best_ev:
                best_ev = eval_metrics.ev_bb_per_100
                best_path = os.path.join(args.save_dir, f'best_ev{best_ev:.2f}')
                learning_trainer.save(best_path)
                logger.info(f"ðŸ’¾ New best model saved! EV: {best_ev:.2f} BB/100")

            # Check convergence
            if evaluator.total_hands >= args.convergence_hands:
                conv, msg = evaluator.check_convergence(
                    learning_id,
                    min_hands=args.convergence_hands,
                    window=1000,
                    target_ev=args.target_ev,
                    tolerance=0.3
                )

                if conv and not converged:
                    logger.info("\n" + "="*70)
                    logger.info("ðŸŽ‰ CONVERGENCE ACHIEVED! ðŸŽ‰")
                    logger.info(f"Agent consistently beats GTO with {best_ev:.2f} BB/100")
                    logger.info("="*70)

                    converged = True

                    # Save converged model
                    final_path = os.path.join(args.save_dir, 'converged_exploitative')
                    learning_trainer.save(final_path)

                    # Generate plots
                    plot_path = os.path.join(args.results_dir, 'final_performance.png')
                    evaluator.plot_performance(learning_id, save_path=plot_path)

                    # Save results
                    results_path = os.path.join(args.results_dir, 'results.json')
                    evaluator.save_results(results_path)

                    logger.info(f"Results saved to {args.results_dir}")
                    break

    # Final evaluation
    logger.info("\n" + "="*70)
    logger.info("FINAL EVALUATION")
    logger.info("="*70)

    final_metrics = evaluate_vs_gto(
        env, learning_trainer, gto_pool, evaluator,
        n_hands=2000,
        learning_id=learning_id,
        logger=logger,
        action_logger=action_logger
    )

    # Save final actions CSV
    final_csv = action_logger.save_to_csv('actions_final.csv')
    logger.info(f"\nFinal actions CSV: {final_csv}")

    # Print action statistics
    stats = action_logger.get_action_statistics()
    logger.info("\nAction Statistics:")
    logger.info(f"  Total hands logged: {stats.get('total_hands', 0)}")
    logger.info(f"  Total actions: {stats.get('total_actions', 0)}")
    logger.info(f"  Actions per hand: {stats.get('actions_per_hand', 0):.2f}")

    if 'action_frequencies' in stats:
        logger.info("\n  Action Frequencies:")
        for action, freq in stats['action_frequencies'].items():
            logger.info(f"    {action}: {freq:.2%}")

    # Compare strategies
    comparison = action_logger.compare_strategies()
    logger.info("\n  GTO vs Learning Agent:")
    logger.info(f"    GTO actions: {comparison.get('total_gto_actions', 0)}")
    logger.info(f"    Learning actions: {comparison.get('total_learning_actions', 0)}")

    if 'frequency_differences' in comparison:
        logger.info("\n  Frequency Differences (Learning - GTO):")
        for action, diff in comparison['frequency_differences'].items():
            sign = "+" if diff > 0 else ""
            logger.info(f"    {action}: {sign}{diff:.2%}")

    logger.info(f"\nFinal EV: {final_metrics.ev_bb_per_100:.2f} BB/100")
    logger.info(f"Target: {args.target_ev} BB/100")

    if final_metrics.ev_bb_per_100 >= args.target_ev:
        logger.info("âœ“ TARGET ACHIEVED!")
    else:
        logger.info(f"âœ— Fell short by {args.target_ev - final_metrics.ev_bb_per_100:.2f} BB/100")

    # Save everything
    plot_path = os.path.join(args.results_dir, 'final_performance.png')
    evaluator.plot_performance(learning_id, save_path=plot_path)

    results_path = os.path.join(args.results_dir, 'final_results.json')
    evaluator.save_results(results_path)

    logger.info("\nTraining complete!")


if __name__ == '__main__':
    args = parse_args()
    train(args)
