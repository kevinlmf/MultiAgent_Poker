# Safe Multi-Agent Poker with GTO Exploitation

This repository implements a **multi-agent reinforcement learning system** for **exploitative poker AI** using **PyTorch**.
The goal is to train agents that achieve **3 BB/100 hands expected value** against **GTO (Game Theory Optimal) players** through adaptive exploitation.

The system demonstrates a **research-grade poker AI architecture** with:
- MADAC (Multi-Agent Dual Actor-Critic) algorithm
- GTO baseline opponent modeling
- Real-time EV tracking with BB/100 metrics
- Comprehensive performance evaluation (Sharpe ratio, drawdown analysis)
- Detailed action logging for strategy analysis

---

##  Quick Start

```bash
# Clone repository
git clone https://github.com/yourusername/Safe_Multi_Agent_Poker
cd Safe_Multi_Agent_Poker

# Setup Python environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run training (recommended test run first)
python training/train_exploitative.py \
  --n_players 3 \
  --max_episodes 100 \
  --eval_interval 50 \
  --eval_hands 100 \
  --target_ev 2.0

# Analyze results
# Training logs and action CSVs are stored in logs/
```

---

## ğŸ“ Project Structure

```
Safe_Multi_Agent_Poker/
â”œâ”€â”€ agents/                   # RL agents
â”‚   â”œâ”€â”€ madac_agent.py            # MADAC algorithm implementation
â”‚   â””â”€â”€ gto_agent.py              # GTO baseline opponent
â”‚
â”œâ”€â”€ environment/              # Poker environment
â”‚   â”œâ”€â”€ poker_env.py              # Multi-agent poker game logic
â”‚   â”œâ”€â”€ hand_evaluator.py         # Hand ranking and comparison
â”‚   â””â”€â”€ game_state.py             # Game state representation
â”‚
â”œâ”€â”€ evaluation/               # Performance evaluation
â”‚   â”œâ”€â”€ ev_calculator.py          # EV and BB/100 tracking
â”‚   â”œâ”€â”€ metrics.py                # Sharpe ratio, drawdown, position analysis
â”‚   â””â”€â”€ convergence_detector.py   # Automatic convergence detection
â”‚
â”œâ”€â”€ training/                 # Training scripts
â”‚   â”œâ”€â”€ train_exploitative.py     # Main exploitative training
â”‚   â””â”€â”€ train_gto.py              # GTO baseline training
â”‚
â”œâ”€â”€ logging/                  # Action logging
â”‚   â”œâ”€â”€ action_logger.py          # CSV export for actions
â”‚   â””â”€â”€ strategy_analyzer.py      # GTO vs Learning comparison
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ gto_theory.md             # GTO explanation
â”‚   â”œâ”€â”€ exploitative_strategy.md  # Exploitation techniques
â”‚   â””â”€â”€ training_guide.md         # Training best practices
â”‚
â”œâ”€â”€ logs/                     # Training outputs
â”‚   â”œâ”€â”€ actions/                  # Action CSV files
â”‚   â””â”€â”€ checkpoints/              # Model checkpoints
â”‚
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

---

## Current Implementation

### 1. **Multi-Agent Training System**
- PyTorch-based implementation
- MADAC (Multi-Agent Dual Actor-Critic) algorithm
- GTO baseline opponents for exploitation learning
- Self-play and opponent modeling

### 2. **EV Evaluation Framework**
- Real-time BB/100 (Big Blinds per 100 hands) tracking
- Sharpe ratio calculation for risk-adjusted performance
- Maximum drawdown monitoring
- Position-based analysis (BTN, SB, BB)
- Automatic convergence detection

### 3. **Action Logging & Analysis**
- CSV format export for all actions
- Detailed game state recording
- GTO vs Exploitative strategy comparison
- Statistical analysis tools

### 4. **Theory & Documentation**
- Comprehensive GTO theory explanation
- Exploitative strategy techniques
- Training guide and best practices

---

##  Future Directions

### 1. **JAX Integration**
- Migrate from PyTorch to JAX for improved computational efficiency
- Leverage hardware acceleration (GPU/TPU) and automatic differentiation
- Reduce training time through optimized tensor operations
- Enable larger-scale multi-agent experiments

### 2. **Representation Learning**
- Deep feature extraction from poker game states
- Advanced information abstraction for hand ranges


### 3. **Extended Game Types**
- Support for different poker variants (Hold'em, Omaha, Stud)
- Variable player counts (2-10 players) and stack sizes
- Tournament structures and ICM (Independent Chip Model) considerations
- Multi-table tournament (MTT) scenarios
- Cash game vs tournament dynamics

---

##  References

This project implements techniques from:

**MADAC (Multi-Agent Dual Actor-Critic)**:
Mao et al. (2020). Multi-Agent Dual Actor-Critic for Cooperative Multi-Agent Reinforcement Learning.

**GTO Poker Theory**:
Mathematics of Poker by Bill Chen and Jerrod Ankenman.

**Exploitative Play**:
Brown & Sandholm (2019). Superhuman AI for multiplayer poker. Science, 365(6456).

---



